{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 9: Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Libaries needed: scikit-surprise, pandas, sklearn, numpy. \n",
    "To install `scikit-surprise`:**\n",
    "```\n",
    "conda install -c conda-forge scikit-surprise\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will be proceeding in two stages. The first stage is where we get into the details of how to build our own recommender system to recommend movies to users. In the second stage, we will be an existing library, specialized for recommender systems, which provides more powerful options. We will be testing it on the task of recommending jokes to users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we have all the requirements ready. In this exercise, you should be filling the empty code sections, marked as `TODO:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 1: Exploring the MovieLens dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll be using the [MovieLens dataset](https://grouplens.org/datasets/movielens/). This dataset is based on [movielens.org](https://movielens.org/), a site where users can get movie recommendations.\n",
    "\n",
    "Our first step is to load the relevant file of the dataset, which you can find in the file `u.data` (on the path `data/ml-100k/u.data`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('data/ml-100k/u.data', sep='\\t', names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>305</td>\n",
       "      <td>451</td>\n",
       "      <td>3</td>\n",
       "      <td>886324817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>883603013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>62</td>\n",
       "      <td>257</td>\n",
       "      <td>2</td>\n",
       "      <td>879372434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>286</td>\n",
       "      <td>1014</td>\n",
       "      <td>5</td>\n",
       "      <td>879781125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>222</td>\n",
       "      <td>5</td>\n",
       "      <td>876042340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>210</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>891035994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>224</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>888104457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  rating  timestamp\n",
       "0       196      242       3  881250949\n",
       "1       186      302       3  891717742\n",
       "2        22      377       1  878887116\n",
       "3       244       51       2  880606923\n",
       "4       166      346       1  886397596\n",
       "5       298      474       4  884182806\n",
       "6       115      265       2  881171488\n",
       "7       253      465       5  891628467\n",
       "8       305      451       3  886324817\n",
       "9         6       86       3  883603013\n",
       "10       62      257       2  879372434\n",
       "11      286     1014       5  879781125\n",
       "12      200      222       5  876042340\n",
       "13      210       40       3  891035994\n",
       "14      224       29       3  888104457"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the number of users and movies in the dataset to get an idea of the scale we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 943 | Number of movies = 1682\n"
     ]
    }
   ],
   "source": [
    "# TODO: get the number of users and itens\n",
    "n_users = len(df['user_id'].unique())\n",
    "n_items = len(df['item_id'].unique())\n",
    "print ('Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get an overall view of the dataset as below. Notice how the ratings range from a minimum of 1 to a maximum of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>462.48475</td>\n",
       "      <td>425.530130</td>\n",
       "      <td>3.529860</td>\n",
       "      <td>8.835289e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>266.61442</td>\n",
       "      <td>330.798356</td>\n",
       "      <td>1.125674</td>\n",
       "      <td>5.343856e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.747247e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>254.00000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.794487e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.00000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.828269e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>682.00000</td>\n",
       "      <td>631.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.882600e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>943.00000</td>\n",
       "      <td>1682.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.932866e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id        item_id         rating     timestamp\n",
       "count  100000.00000  100000.000000  100000.000000  1.000000e+05\n",
       "mean      462.48475     425.530130       3.529860  8.835289e+08\n",
       "std       266.61442     330.798356       1.125674  5.343856e+06\n",
       "min         1.00000       1.000000       1.000000  8.747247e+08\n",
       "25%       254.00000     175.000000       3.000000  8.794487e+08\n",
       "50%       447.00000     322.000000       4.000000  8.828269e+08\n",
       "75%       682.00000     631.000000       4.000000  8.882600e+08\n",
       "max       943.00000    1682.000000       5.000000  8.932866e+08"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded, we proceed to splitting it into a training set and a testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.25, random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the user-item matrices, one for training and another for testing. Each matrix should be a 2D numpy array, with each row corresponding to a user and each column to a movie. A non-zero cell in the matrix is the rating given by the user to the movie (zeros are for the case of no corresponding rating).\n",
    "\n",
    "**Notice that the user ids and item ids start from 1, so the index (0,0) in your matrix should correspond to `user_id` of 1 and `item_id` of 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fill the code to produce a data matrix\n",
    "def create_data_matrix(data,n_users,n_items):\n",
    "    \"\"\"\n",
    "        This function should return a numpy matrix with a shape (n_users, n_items). \n",
    "        Each entry is the rating given by the user to the item\n",
    "    \"\"\"\n",
    "    data_matrix = np.zeros([n_users, n_items])\n",
    "    for index, row in data.iterrows():\n",
    "        data_matrix[row['user_id']-1,row['item_id']-1] = row['rating']\n",
    "    return data_matrix\n",
    "\n",
    "train_data_matrix= create_data_matrix(train_data, n_users, n_items)\n",
    "test_data_matrix= create_data_matrix(test_data, n_users, n_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how our matrices look like at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_matrix\n",
      "[[0. 3. 4. ... 0. 0. 0.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [5. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 5. 0. ... 0. 0. 0.]]\n",
      "test_data_matrix\n",
      "[[5. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('train_data_matrix')\n",
    "print(train_data_matrix)\n",
    "print('test_data_matrix')\n",
    "print(test_data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've prepared our data, the next mission we have is to create a recommender system following the paradigm of Item-based Collaborative Filtering. In this case, this is translated into \"Users who liked this item (movie) also liked â€¦\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make predictions, we will apply following formula, where \n",
    "$N_I(a)$ is the set of neighbors of item $a$, and $b$ is an item rated by user $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "{r}_{x}(a) =  \\frac{\\sum\\limits_{b \\in N_{I}(a)} sim(a, b) r_{x}(b)}{\\sum\\limits_{b \\in N_{I}(a)}|sim(a, b)|}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a building block, we'll first write the code for the similarity $sim(a,b)$ metric between each two item vectors in our training matrix. In this case, we will use the cosine similarity metric. The output should be an `n_items` by `n_items` symmetric 2D numpy matrix with the similarity between each couple of items.\n",
    "\n",
    "**Note**: In this exercise, there are always two ways of achieving the same goal: a slow one via `for` loops and another by benefiting from numpy's speed in matrix operations. Feel free to improve your starting solution to a faster one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.70568037 0.74751901 ... 1.         1.         1.        ]\n",
      " [0.70568037 0.         0.81144044 ... 1.         0.90900731 1.        ]\n",
      " [0.74751901 0.81144044 0.         ... 1.         1.         1.        ]\n",
      " ...\n",
      " [1.         1.         1.         ... 0.         1.         1.        ]\n",
      " [1.         0.90900731 1.         ... 1.         0.         1.        ]\n",
      " [1.         1.         1.         ... 1.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TODO fill the code to compute the similarity matrix\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "item_similarity = pairwise_distances(np.transpose(train_data_matrix),metric=\"cosine\")\n",
    "\n",
    "# check how the matrix looks like\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the similarity matrix in the above formula to obtain the predicted ranking for each item `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 3. 4. ... 4. 4. 4.]\n",
      " [4. 4. 4. ... 4. 4. 4.]\n",
      " [3. 3. 3. ... 3. 3. 3.]\n",
      " ...\n",
      " [5. 4. 4. ... 4. 4. 4.]\n",
      " [4. 4. 4. ... 4. 4. 4.]\n",
      " [3. 5. 3. ... 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fill the code for predicting the ratings. \n",
    "# The output is a numpy matrix with the dimensions ((n_users,n_items)) and with the corresponding ranking at each cell.\n",
    "def item_based_predict(ratings, similarity):\n",
    "    pred = np.zeros([ratings.shape[0],ratings.shape[1]])\n",
    "    for x in range(pred.shape[0]):\n",
    "        \n",
    "        notZerosIndex = np.where(ratings[x,:] != 0)\n",
    "        \n",
    "        for y in range(pred.shape[1]):\n",
    "            if(ratings[x,y]==0):\n",
    "                relativeSim = (similarity[notZerosIndex,y])\n",
    "                predRatings = np.dot(ratings[x,notZerosIndex],np.transpose(relativeSim)) / np.sum(relativeSim)\n",
    "                pred[x,y] = np.round(predRatings)\n",
    "            else:\n",
    "                pred[x,y] = ratings[x,y]\n",
    "    return pred\n",
    "\n",
    "item_prediction = item_based_predict(train_data_matrix, item_similarity)\n",
    "print(item_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: User-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next mission we have is to create a recommender system following the paradigm of User-based Collaborative Filtering. In this case, this is translated into \"Users who are similar to you also likedâ€¦\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make predictions, we will apply following formula, where $N_U(x)$ is the set of neighbors of user x and $a$ is an item not rated by x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "{r}_{x}(a) = \\bar{r}_{x} + \\frac{\\sum\\limits_{y \\in N_{U}(x)} sim(x, y) (r_{y}(a) - \\bar{r}_{y})}{\\sum\\limits_{y \\in N_{U}(x)}|sim(x, y)|}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, we will first compute the distances between the users in our training matrix, using cosine similarity. The output should be an `n_users` by `n_users` symmetric 2D numpy matrix with the similarity between each couple of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 943)\n",
      "[[0.         0.85663074 0.96758314 ... 0.9103956  0.91215203 0.68584107]\n",
      " [0.85663074 0.         0.89240931 ... 0.91889238 0.85429877 0.92022661]\n",
      " [0.96758314 0.89240931 0.         ... 0.97613014 0.89296834 1.        ]\n",
      " ...\n",
      " [0.9103956  0.91889238 0.97613014 ... 0.         0.93055179 0.93272018]\n",
      " [0.91215203 0.85429877 0.89296834 ... 0.93055179 0.         0.8828355 ]\n",
      " [0.68584107 0.92022661 1.         ... 0.93272018 0.8828355  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TODO fill the code to compute the similarity matrix\n",
    "user_similarity = pairwise_distances(train_data_matrix,metric=\"cosine\")\n",
    "\n",
    "# print the shape as a sanity check\n",
    "print(user_similarity.shape)\n",
    "\n",
    "# check how the matrix looks like\n",
    "print(user_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 3. 4. ... 2. 4. 4.]\n",
      " [4. 4. 3. ... 3. 4. 4.]\n",
      " [3. 3. 2. ... 2. 3. 3.]\n",
      " ...\n",
      " [5. 4. 4. ... 3. 4. 4.]\n",
      " [5. 4. 4. ... 3. 4. 4.]\n",
      " [4. 5. 3. ... 2. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fill the code for predicting the ratings. \n",
    "import math\n",
    "\n",
    "def user_based_predict(ratings, similarity):\n",
    "    \n",
    "    pred = np.zeros([ratings.shape[0],ratings.shape[1]])\n",
    "    ratings[ratings == 0] = np.nan\n",
    "    averageRating = np.nanmean(ratings,axis=1)\n",
    "\n",
    "    for x in range(pred.shape[0]):\n",
    "        for y in range(pred.shape[1]):\n",
    "            if(np.isnan(ratings[x,y])):\n",
    "                notZerosUsers = np.where(~np.isnan(ratings[:,y]))\n",
    "                if (len(notZerosUsers[0]) > 0):\n",
    "                    relativeSim = (similarity[notZerosUsers,x])\n",
    "                    relativeAvg = (averageRating[notZerosUsers])\n",
    "                    relativeAvg = relativeAvg.reshape([1, relativeAvg.shape[0]])\n",
    "                    predRatings = np.dot((ratings[notZerosUsers,y] - relativeAvg), np.transpose(relativeSim)) / np.sum(relativeSim)\n",
    "                    pred[x,y] = np.round(averageRating[x] + predRatings) \n",
    "                else:\n",
    "                    pred[x,y] = np.round(averageRating[x])\n",
    "            else:\n",
    "                pred[x,y] = ratings[x,y]\n",
    "    return pred\n",
    "\n",
    "user_prediction = user_based_predict(train_data_matrix, user_similarity)\n",
    "print(user_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Evaluating Our Recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be evaluating our recommenders using Root Mean Squared Error (RMSE). In the formula below, $r_i$ is the true rating and $\\hat{r_i}$ is the predicted one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathit{RMSE} =\\sqrt{\\frac{1}{N} \\sum_i (r_i -\\hat{r_i})^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 1.0073926741842032\n",
      "Item-based CF RMSE: 1.090247678282325\n"
     ]
    }
   ],
   "source": [
    "# TODO: add the code for computing RMSE for user and item based methods\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    y_pred = []\n",
    "    y_truth = []\n",
    "    n_user = ground_truth.shape[0]\n",
    "    n_item = ground_truth.shape[1]\n",
    "    for i in range(n_user):\n",
    "        for j in range(n_item):\n",
    "            if ground_truth[i,j] != 0 :\n",
    "                y_pred.append(prediction[i,j])\n",
    "                y_truth.append(ground_truth[i,j])\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_truth = np.array(y_truth)\n",
    "    \n",
    "    rmse_val = sqrt(mean_squared_error(y_truth,y_pred))\n",
    "    return rmse_val\n",
    "\n",
    "print ('User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n",
    "print ('Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Introducing Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will move to using [Surprise](http://surpriselib.com/), a full-fledged python library, specialized for recommender systems. The goal is to get exposed to such more powerful libraries that can automate a lot of the manual work we had to do above.\n",
    "\n",
    "For a change, we will be using the [Jester](http://eigentaste.berkeley.edu/dataset/) dataset, obtained from the [Jester Online Joke Recommender System](http://eigentaste.berkeley.edu/index.html). It has over 1.7 million continuous ratings (-10.00 to +10.00) of 150 jokes from 59,132 users: collected between November 2006 - May 2009. Our first step will be to download this dataset. Fortunately, `Surprise` has a built-in loader for the Jester dataset. Make sure you confirm that you want to download the dataset when prompted to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset jester could not be found. Do you want to download it? [Y/n] Y\n",
      "Trying to download dataset from http://eigentaste.berkeley.edu/dataset/jester_dataset_2.zip...\n",
      "Done! Dataset jester has been saved to /Users/sanadhisutandi/.surprise_data/jester\n"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "# Load the Jester dataset (download it if needed),\n",
    "data = Dataset.load_builtin('jester')\n",
    "# split the data into 2 folds for cross-validation.\n",
    "data.split(n_folds=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to train the k-Nearest Neighbors algorithm within Surprise on the Jester dataset (Check the [documentation](http://surprise.readthedocs.io/en/stable/) for `SVD`). For evaluation, Jester allows multiple metrics. You will need to use the `RMSE` and the `MAE` in this case. The training might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanadhisutandi/anaconda/lib/python3.6/site-packages/surprise/evaluate.py:66: UserWarning: The evaluate() method is deprecated. Please use model_selection.cross_validate() instead.\n",
      "  'model_selection.cross_validate() instead.', UserWarning)\n",
      "/Users/sanadhisutandi/anaconda/lib/python3.6/site-packages/surprise/dataset.py:193: UserWarning: Using data.split() or using load_from_folds() without using a CV iterator is now deprecated. \n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 4.3877\n",
      "MAE:  3.3202\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 4.3712\n",
      "MAE:  3.3107\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 4.3794\n",
      "Mean MAE : 3.3154\n",
      "------------\n",
      "------------\n",
      "        Fold 1  Fold 2  Mean    \n",
      "RMSE    4.3877  4.3712  4.3794  \n",
      "MAE     3.3202  3.3107  3.3154  \n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import evaluate, print_perf\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# TODO: fill the code for evaluating the model based on SVD\n",
    "# We'll use the SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset.\n",
    "perf = evaluate(algo, data, measures=['RMSE', 'MAE'])\n",
    "\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code was hopefully short, and it's mainly for showing the power of the library. Now that you have trained and evaluated the recommendation algorithm, let's try to find the predicted rating for a single user and item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.4395\n",
      "RMSE: 4.4288\n",
      "RMSE: 4.4304\n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import KFold\n",
    "from surprise import accuracy\n",
    "\n",
    "uid = str(196)  # raw user id (as in the ratings file). They are **strings**!\n",
    "iid = str(98)  # raw item id (as in the ratings file). They are **strings**!\n",
    "\n",
    "predictions = None\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "for trainset, testset in kf.split(Dataset.load_builtin('jester')):\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "pred = algo.predict(uid,iid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in knowing what the joke was for item 98, you can check the dataset. By default, the dataset will be downloaded in your home directory, under `$HOME/.surprise_data/jester/`. The file `jester_items.dat` has the text of the jokes. ðŸ˜‰\n",
    "\n",
    "Finally, feel free to explore the library further. It might come in handy for your future projects!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid='196', iid='98', r_ui=None, est=-1.8517835503534652, details={'was_impossible': False})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
