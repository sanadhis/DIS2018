{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Advanced Information Retrieval\n",
    "\n",
    "## Question 1 - Latent Semantic Indexing\n",
    "\n",
    "\n",
    "In this exercise, we will run latent semantic indexing on a term-document matrix using python numpy library.\n",
    "\n",
    "Suppose we are given the following term-document matrix containing eleven terms and four documents $d_1$ , $d_2$ , $d_3$ and $d_4$:\n",
    "\n",
    "$\n",
    "M =\n",
    "  \\begin{bmatrix}\n",
    "    d_1 & d_2 & d_3 & d_4 \\\\ \n",
    "\t1 & 1 & 1 & 1  \\\\\n",
    "\t0 & 1 & 1 & 1 \\\\\n",
    "\t1 & 0 & 0 & 0 \\\\\n",
    "\t0 & 1 & 0 & 0 \\\\\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    1 & 0 & 1 & 2 \\\\\n",
    "    1 & 1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 & 0 \\\\\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    0 & 2 & 1 & 1 \\\\\n",
    "    0 & 1 & 1 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "###  Question 1.a\n",
    "\n",
    "Compute the singular value decomposition of the term-document matrix M. Print the values of the output matrices $K$, $S$ and $D^t$.\n",
    "\n",
    "\n",
    "<b>Hint:</b> Use the function numpy.linalg.svd. More details of this function can be found here at this link:\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html\n",
    "\n",
    "\n",
    "Here's sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41291701, -0.12294407,  0.05933248, -0.03660797],\n",
       "       [-0.3359611 ,  0.1962311 , -0.25246121,  0.11968319],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115],\n",
       "       [-0.11909604,  0.2663899 ,  0.20432237, -0.52093504],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115],\n",
       "       [-0.39922386, -0.49767812, -0.57172873,  0.04465203],\n",
       "       [-0.41291701, -0.12294407,  0.05933248, -0.03660797],\n",
       "       [-0.30751414, -0.01459992,  0.48607132,  0.40306708],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115],\n",
       "       [-0.45505713,  0.462621  , -0.04813884, -0.40125186],\n",
       "       [-0.23055822,  0.30457524,  0.17427762,  0.55935823]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Python matrix operations library\n",
    "import numpy as np\n",
    "\n",
    "#set M matrix using the given values.\n",
    "M = [[1,1,1,1], \n",
    "     [0,1,1,1],\n",
    "     [1,0,0,0],\n",
    "     [0,1,0,0],\n",
    "     [1,0,0,0],\n",
    "     [1,0,1,2],\n",
    "     [1,1,1,1],\n",
    "     [1,1,1,0],\n",
    "     [1,0,0,0],\n",
    "     [0,2,1,1],\n",
    "     [0,1,1,0]]\n",
    "\n",
    "\n",
    "M = np.array(M)\n",
    "\n",
    "# compute SVD\n",
    "K, S, Dt = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Question 1.b\n",
    "\n",
    "Are the values of $S$ sorted? Perform latent semantic indexing by selecting the first two largest singular values of the matrix $S$.\n",
    "\n",
    "<b>Hint:</b> See the lecture slides on latent semantic indexing for more details. A sub-matrix of a numpy matrix can be computed using indexing operations (see https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.78695453,  2.31848919,  1.762346  ,  0.77705263])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_sel = K[:,:2]\n",
    "S_sel = S[:2]\n",
    "Dt_sel = Dt[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.c\n",
    "\n",
    "Given the query $q$:\n",
    "\n",
    "$\n",
    "q =\n",
    "  \\begin{bmatrix}\n",
    "\t0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "Map query $q$ into the new document space $D$. The new query is referred to as $q^*$. \n",
    "\n",
    "<b>Hint:</b> Use the formulation for mapping queries provided in the lecture slides. You can also use np.linalg.inv function for computing the inverse of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22662409,  0.11624731])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1c\n",
    "# Form q\n",
    "q = np.array([0,0,0,0,0,1,0,0,0,1,1])\n",
    "\n",
    "# Form S\n",
    "S_1 = np.linalg.inv(np.diag(S_sel))\n",
    "\n",
    "# Calculate q_star\n",
    "q_star = (q).dot(K_sel).dot(S_1)\n",
    "q_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.d\n",
    "\n",
    "Arrange the documents based on the cosine similarity measure between $q^*$ and the new documents in the space $D$.\n",
    "\n",
    "<b>Hint:</b> Use the cosine similarity function from the previous exercise on vector space retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0120579132787\n",
      "0.938882772715\n",
      "0.952477624421\n",
      "0.593108626807\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Function for computing cosine similarity.\n",
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy*1.0/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "d1 = Dt_sel[:,0]\n",
    "d2 = Dt_sel[:,1]\n",
    "d3 = Dt_sel[:,2]\n",
    "d4 = Dt_sel[:,3]\n",
    "\n",
    "cos1 = cosine_similarity(q_star,d1)\n",
    "cos2 = cosine_similarity(q_star,d2)\n",
    "cos3 = cosine_similarity(q_star,d3)\n",
    "cos4 = cosine_similarity(q_star,d4)\n",
    "\n",
    "print(cos1) \n",
    "print(cos2)\n",
    "print(cos3)\n",
    "print(cos4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrangement: D3 > D2 > D4 > D1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.e\n",
    "\n",
    "Does the order of documents change if document $d_3$ is dropped? If yes, why? \n",
    "If no, how should $d_3$ be modified to change the document ordering?\n",
    "\n",
    "<br><b>No</b> because D3 is aligned with D2 and D4 (similar magnitude). Removing D3 does not significantly change term space (K) and document space (D) of SVD.\n",
    "\n",
    "<br><b>To change this</b>, define such D3 that goes in different direction compared with D2 and D4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.f [Optional]\n",
    "\n",
    "Run latent semantic indexing for the document collection presented in the previous exercise (presented here as well):\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "Now, for the query $Q=$''<i>baking</i>'', find the top ranked documents according to LSI (use three singular values). \n",
    "\n",
    "<b>Hint:</b> Use the code for computing document_vectors from the last exercise. However note that document_vectors represent document-term matrix whereas LSI uses term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sanadhisutandi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "# with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.transpose(np.array(document_vectors))\n",
    "\n",
    "# compute SVD\n",
    "K, S, Dt = np.linalg.svd(M, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_sel = K[:,:3]\n",
    "S_sel = S[:3]\n",
    "Dt_sel = Dt[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['art',\n",
       " 'bake',\n",
       " 'best',\n",
       " 'book',\n",
       " 'bread',\n",
       " 'cake',\n",
       " 'comput',\n",
       " 'french',\n",
       " 'london',\n",
       " 'numer',\n",
       " 'pastri',\n",
       " 'pie',\n",
       " 'quantiti',\n",
       " 'recip',\n",
       " 'scientif',\n",
       " 'smith',\n",
       " 'without']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forming q\n",
    "# Check Vocabulary\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming q\n",
    "# Forming vector 0 with size of vocabulary\n",
    "q = np.array([0]*len(vocabulary))\n",
    "\n",
    "# bake is 2nd element -> index 1\n",
    "q[1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00416487,  0.11537136, -0.14603541])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Form S\n",
    "S_1 = np.linalg.inv(np.diag(S_sel))\n",
    "\n",
    "# Calculate q_star\n",
    "q_star = (q).dot(K_sel).dot(S_1)\n",
    "q_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998051867877\n",
      "-0.657760956636\n",
      "-0.00232887505297\n",
      "0.723107878968\n",
      "-0.655106291136\n"
     ]
    }
   ],
   "source": [
    "# Calculate using approach in 1d\n",
    "import math\n",
    "\n",
    "# Function for computing cosine similarity.\n",
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy*1.0/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "d1 = Dt_sel[:,0]\n",
    "d2 = Dt_sel[:,1]\n",
    "d3 = Dt_sel[:,2]\n",
    "d4 = Dt_sel[:,3]\n",
    "d5 = Dt_sel[:,4]\n",
    "\n",
    "cos1 = cosine_similarity(q_star,d1)\n",
    "cos2 = cosine_similarity(q_star,d2)\n",
    "cos3 = cosine_similarity(q_star,d3)\n",
    "cos4 = cosine_similarity(q_star,d4)\n",
    "cos5 = cosine_similarity(q_star,d5)\n",
    "\n",
    "print(cos1) \n",
    "print(cos2)\n",
    "print(cos3)\n",
    "print(cos4)\n",
    "print(cos5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "## Question 2 - Word Embeddings\n",
    "\n",
    "In this exercise, we would train word embeddings using a state-of-the-art embeddings library fastText. The first step of the exercise is to install the fastText library. Proceed with the following steps:\n",
    "\n",
    "### FastText installation\n",
    "\n",
    "\n",
    "#### Run these commands on the shell terminal:\n",
    "\n",
    "> wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip <br>\n",
    "> unzip v0.1.0.zip<br>\n",
    "> cd fastText-0.1.0 <br>\n",
    "> make<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Move the epfldocs.txt file (provided in the last exercise) to the current directory. Sample command (linux) for copying the file into current directory is as follows:\n",
    "\n",
    "> cp directory_path/epfldocs.txt ./\n",
    "\n",
    "<br>\n",
    "\n",
    "### Generate Embeddings\n",
    "\n",
    "Further, generate fasttext embeddings for the epfldocs.txt file using the following command:\n",
    "\n",
    "> ./fasttext skipgram -input epfldocs.txt -output model_epfldocs\n",
    "\n",
    "\n",
    "The above command generates word embeddings and stores them in a file named model_epfldocs.vec.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Load Embeddings\n",
    "\n",
    "In the second phase of this exercise, we will load these embeddings into memory using python and visualize them.\n",
    "Use the following python code to load the embeddings into memory:<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_embeddings(file_name):\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "        lines = lines[1:]\n",
    "        vocabulary, wv = zip(*[line.strip().split(' ', 1) for line in lines])\n",
    "    wv = np.loadtxt(wv)\n",
    "    return wv, vocabulary\n",
    "\n",
    "\n",
    "# Replace the path based on your own machine.\n",
    "word_embeddings, vocabulary = load_embeddings(directory_path + 'model_epfldocs.vec')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Visualize Embeddings\n",
    "\n",
    "In the third phase of this exercise, we will visualize the generated embeddings. First install the tsne library using pip:\n",
    "\n",
    "> $ pip install tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsne import bh_sne\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "vis_data = bh_sne(word_embeddings)\n",
    "\n",
    "vis_data_x = vis_data[:,0]\n",
    "vis_data_y = vis_data[:,1]\n",
    "\n",
    "plt.scatter(vis_data_x, vis_data_y)\n",
    "for label, x, y in zip(vocabulary, vis_data_x, vis_data_y):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.a\n",
    "\n",
    "Observe the plot of word embeddings. Do you observe any patterns?\n",
    "\n",
    "\n",
    "\n",
    "### Question 2.b\n",
    "\n",
    "Write a python function to find the most similar terms for a given term. The similarity between two terms is defined as the cosine similarity between their corresponding word embeddings.\n",
    "\n",
    "Find the top 3 terms that are most similar to 'la', 'EPFL', '#robot', 'this', \n",
    "\n",
    "### Question 2.c [Optional]\n",
    "\n",
    "Download the text file using the following command:\n",
    "\n",
    "> wget http://mattmahoney.net/dc/text8.zip -O text8.gz <br>\n",
    "> tar -xvf text8.gz \n",
    "\n",
    "\n",
    "The above command creates a text file named 'text8'. Regenerate the fasttext embeddings using the text8 file. Plot the word embeddings for first 1000 terms in the vocabulary.\n",
    "\n",
    "### Question 2.d [Optional]\n",
    "\n",
    "Observe the word embeddings that are visualized in this link http://www.anthonygarvan.com/wordgalaxy/ . Can you make some interesting observations? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
